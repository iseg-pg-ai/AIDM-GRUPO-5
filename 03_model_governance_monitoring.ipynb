{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88c449a7-5057-4233-988e-fa7cf48eaf5d",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7b4986e-6bb2-43cf-a752-8d3177e4c003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import sagemaker\n",
    "from sagemaker.session import Session\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.model_monitor import DefaultModelMonitor\n",
    "from sagemaker.model_monitor.dataset_format import DatasetFormat\n",
    "from sagemaker.model_monitor import CronExpressionGenerator\n",
    "from sagemaker.model_monitor import EndpointInput\n",
    "\n",
    "region = sagemaker.Session().boto_region_name\n",
    "role = get_execution_role()\n",
    "sm_session = Session()\n",
    "sm_client = boto3.client(\"sagemaker\", region_name=region)\n",
    "\n",
    "bucket = \"i32419\"\n",
    "prefix = \"ai-deployment-monitoring-grupo-5\"\n",
    "\n",
    "endpoint_name = \"loan-default-endpoint-2026-01-11-23-28-57\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9dd2b6-1811-466a-872c-80b2f03548e9",
   "metadata": {},
   "source": [
    "## Model Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "37195b8d-d800-4b02-8df5-0aa5a79d3d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_card.model_card:Creating model card with name: aidm-grupo-5-loan-default-byoc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create failed (likely exists). Updating instead. Error: An error occurred (ConflictException) when calling the CreateModelCard operation: Modelcard arn:aws:sagemaker:eu-west-1:267567228900:model-card/aidm-grupo-5-loan-default-byoc with version 1 already ex\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭─────────────────────────────── </span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Traceback </span><span style=\"color: #ff7f7f; text-decoration-color: #ff7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> in &lt;module&gt;:54                                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">51 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">52 # cria ou atualiza</span>                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">53 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">try</span>:                                                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>54 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>card.create()                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">55 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Created Model Card:\"</span>, card.arn)                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">56 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">Exception</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> e:                                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">57 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># se já existir, faz update</span>                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/opt/conda/lib/python3.12/site-packages/sagemaker/model_card/</span><span style=\"font-weight: bold\">model_card.py</span>:1530 in create        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1527 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>request_args = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._create_request_args()                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1528 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>logger.info(<span style=\"color: #808000; text-decoration-color: #808000\">\"Creating model card with name: %s\"</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.name)                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1529 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>logger.debug(<span style=\"color: #808000; text-decoration-color: #808000\">\"CreateModelCard request: %s\"</span>, json.dumps(request_args, indent=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">4</span>))   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1530 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">.sagemaker_session.sagemaker_client.create_model_card(**request_args)</span>         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1531 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1532 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># udpate model card with the latest data from server</span>                              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1533 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>response = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.sagemaker_session.sagemaker_client.describe_model_card(           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/opt/conda/lib/python3.12/site-packages/botocore/</span><span style=\"font-weight: bold\">client.py</span>:569 in _api_call                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 566 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">f\"{</span>py_operation_name<span style=\"color: #808000; text-decoration-color: #808000\">}() only accepts keyword arguments.\"</span>              <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 567 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>)                                                                         <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 568 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># The \"self\" in this scope is referring to the BaseClient.</span>                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 569 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">self</span><span style=\"font-weight: bold; text-decoration: underline\">._make_api_call(operation_name, kwargs)</span>                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 570 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 571 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>_api_call.<span style=\"color: #ff0000; text-decoration-color: #ff0000\">__name__</span> = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(py_operation_name)                                       <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 572 </span>                                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/opt/conda/lib/python3.12/site-packages/botocore/</span><span style=\"font-weight: bold\">client.py</span>:1023 in _make_api_call                <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1020 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"Code\"</span>                                                                    <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1021 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>)                                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1022 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>error_class = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.exceptions.from_code(error_code)                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1023 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">raise</span><span style=\"font-weight: bold; text-decoration: underline\"> error_class(parsed_response, operation_name)</span>                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1024 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> parsed_response                                                        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1026 </span>                                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ConflictException: </span>An error occurred <span style=\"font-weight: bold\">(</span>ConflictException<span style=\"font-weight: bold\">)</span> when calling the CreateModelCard operation: Modelcard \n",
       "arn:aws:sagemaker:eu-west-1:267567228900:model-card/aidm-grupo-<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span>-loan-default-byoc with version <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span> already exists\n",
       "\n",
       "<span style=\"font-style: italic\">During handling of the above exception, another exception occurred:</span>\n",
       "\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╭─────────────────────────────── </span><span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">Traceback </span><span style=\"color: #ff7f7f; text-decoration-color: #ff7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #ff0000; text-decoration-color: #ff0000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> in &lt;module&gt;:59                                                                                   <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">56 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">except</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">Exception</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> e:                                                                      <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">57 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># se já existir, faz update</span>                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">58 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Create failed (likely exists). Updating instead. Error:\"</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>(e)[:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">200</span>])          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>59 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"font-weight: bold; text-decoration: underline\">card.update()</span>                                                                           <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">60 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">print</span>(<span style=\"color: #808000; text-decoration-color: #808000\">\"Updated Model Card:\"</span>, card.arn)                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">61 </span>                                                                                            <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">/opt/conda/lib/python3.12/site-packages/sagemaker/model_card/</span><span style=\"font-weight: bold\">model_card.py</span>:1642 in update        <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>                                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1639 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1640 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># use created_time to infer if the model card</span>                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1641 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.created_time <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">is</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>:                                                     <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1642 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff; font-weight: bold; text-decoration: underline\">raise</span><span style=\"font-weight: bold; text-decoration: underline\"> </span><span style=\"color: #00ffff; text-decoration-color: #00ffff; font-weight: bold; text-decoration: underline\">ValueError</span><span style=\"font-weight: bold; text-decoration: underline\">(</span>                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1643 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold; text-decoration: underline\">\"Please create a model card or load an existing model card before update</span>  <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1644 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"font-weight: bold; text-decoration: underline\">)</span>                                                                             <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1645 </span>                                                                                          <span style=\"color: #ff0000; text-decoration-color: #ff0000\">│</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">ValueError: </span>Please create a model card or load an existing model card before update.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[38;2;255;0;0m╭─\u001b[0m\u001b[38;2;255;0;0m──────────────────────────────\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0mTraceback \u001b[0m\u001b[1;2;38;2;255;0;0m(most recent call last)\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[38;2;255;0;0m───────────────────────────────\u001b[0m\u001b[38;2;255;0;0m─╮\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m in <module>:54                                                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m51 \u001b[0m                                                                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m52 \u001b[0m\u001b[2m# cria ou atualiza\u001b[0m                                                                          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m53 \u001b[0m\u001b[94mtry\u001b[0m:                                                                                        \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m54 \u001b[2m│   \u001b[0mcard.create()                                                                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m55 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mCreated Model Card:\u001b[0m\u001b[33m\"\u001b[0m, card.arn)                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m56 \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mException\u001b[0m \u001b[94mas\u001b[0m e:                                                                      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m57 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# se já existir, faz update\u001b[0m                                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2m/opt/conda/lib/python3.12/site-packages/sagemaker/model_card/\u001b[0m\u001b[1mmodel_card.py\u001b[0m:1530 in create        \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1527 \u001b[0m\u001b[2m│   │   \u001b[0mrequest_args = \u001b[96mself\u001b[0m._create_request_args()                                        \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1528 \u001b[0m\u001b[2m│   │   \u001b[0mlogger.info(\u001b[33m\"\u001b[0m\u001b[33mCreating model card with name: \u001b[0m\u001b[33m%s\u001b[0m\u001b[33m\"\u001b[0m, \u001b[96mself\u001b[0m.name)                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1529 \u001b[0m\u001b[2m│   │   \u001b[0mlogger.debug(\u001b[33m\"\u001b[0m\u001b[33mCreateModelCard request: \u001b[0m\u001b[33m%s\u001b[0m\u001b[33m\"\u001b[0m, json.dumps(request_args, indent=\u001b[94m4\u001b[0m))   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m1530 \u001b[2m│   │   \u001b[0m\u001b[1;4;96mself\u001b[0m\u001b[1;4m.sagemaker_session.sagemaker_client.create_model_card(**request_args)\u001b[0m         \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1531 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1532 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# udpate model card with the latest data from server\u001b[0m                              \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1533 \u001b[0m\u001b[2m│   │   \u001b[0mresponse = \u001b[96mself\u001b[0m.sagemaker_session.sagemaker_client.describe_model_card(           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2m/opt/conda/lib/python3.12/site-packages/botocore/\u001b[0m\u001b[1mclient.py\u001b[0m:569 in _api_call                      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 566 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0mpy_operation_name\u001b[33m}\u001b[0m\u001b[33m() only accepts keyword arguments.\u001b[0m\u001b[33m\"\u001b[0m              \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 567 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 568 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m 569 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[1;4;96mself\u001b[0m\u001b[1;4m._make_api_call(operation_name, kwargs)\u001b[0m                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 570 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 571 \u001b[0m\u001b[2m│   │   \u001b[0m_api_call.\u001b[91m__name__\u001b[0m = \u001b[96mstr\u001b[0m(py_operation_name)                                       \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m 572 \u001b[0m                                                                                          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2m/opt/conda/lib/python3.12/site-packages/botocore/\u001b[0m\u001b[1mclient.py\u001b[0m:1023 in _make_api_call                \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1020 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mCode\u001b[0m\u001b[33m\"\u001b[0m                                                                    \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1021 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1022 \u001b[0m\u001b[2m│   │   │   \u001b[0merror_class = \u001b[96mself\u001b[0m.exceptions.from_code(error_code)                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m1023 \u001b[2m│   │   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m error_class(parsed_response, operation_name)\u001b[0m                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1024 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1025 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m parsed_response                                                        \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1026 \u001b[0m                                                                                          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mConflictException: \u001b[0mAn error occurred \u001b[1m(\u001b[0mConflictException\u001b[1m)\u001b[0m when calling the CreateModelCard operation: Modelcard \n",
       "arn:aws:sagemaker:eu-west-1:267567228900:model-card/aidm-grupo-\u001b[1;36m5\u001b[0m-loan-default-byoc with version \u001b[1;36m1\u001b[0m already exists\n",
       "\n",
       "\u001b[3mDuring handling of the above exception, another exception occurred:\u001b[0m\n",
       "\n",
       "\u001b[38;2;255;0;0m╭─\u001b[0m\u001b[38;2;255;0;0m──────────────────────────────\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[1;38;2;255;0;0mTraceback \u001b[0m\u001b[1;2;38;2;255;0;0m(most recent call last)\u001b[0m\u001b[38;2;255;0;0m \u001b[0m\u001b[38;2;255;0;0m───────────────────────────────\u001b[0m\u001b[38;2;255;0;0m─╮\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m in <module>:59                                                                                   \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m56 \u001b[0m\u001b[94mexcept\u001b[0m \u001b[96mException\u001b[0m \u001b[94mas\u001b[0m e:                                                                      \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m57 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# se já existir, faz update\u001b[0m                                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m58 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mCreate failed (likely exists). Updating instead. Error:\u001b[0m\u001b[33m\"\u001b[0m, \u001b[96mstr\u001b[0m(e)[:\u001b[94m200\u001b[0m])          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m59 \u001b[2m│   \u001b[0m\u001b[1;4mcard.update()\u001b[0m                                                                           \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m60 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mUpdated Model Card:\u001b[0m\u001b[33m\"\u001b[0m, card.arn)                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m61 \u001b[0m                                                                                            \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[2m/opt/conda/lib/python3.12/site-packages/sagemaker/model_card/\u001b[0m\u001b[1mmodel_card.py\u001b[0m:1642 in update        \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m                                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1639 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1640 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# use created_time to infer if the model card\u001b[0m                                     \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1641 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.created_time \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                     \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m \u001b[31m❱ \u001b[0m1642 \u001b[2m│   │   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m \u001b[0m\u001b[1;4;96mValueError\u001b[0m\u001b[1;4m(\u001b[0m                                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1643 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[1;4;33m\"\u001b[0m\u001b[1;4;33mPlease create a model card or load an existing model card before update\u001b[0m  \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1644 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[1;4m)\u001b[0m                                                                             \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m│\u001b[0m   \u001b[2m1645 \u001b[0m                                                                                          \u001b[38;2;255;0;0m│\u001b[0m\n",
       "\u001b[38;2;255;0;0m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mValueError: \u001b[0mPlease create a model card or load an existing model card before update.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sagemaker.model_card import (\n",
    "    ModelCard,\n",
    "    ModelCardStatusEnum,\n",
    "    ModelOverview,\n",
    "    IntendedUses,\n",
    "    TrainingDetails,\n",
    "    ObjectiveFunction,\n",
    "    Function,\n",
    "    ObjectiveFunctionEnum,\n",
    "    FacetEnum,\n",
    "    RiskRatingEnum,\n",
    ")\n",
    "\n",
    "model_card_name = \"aidm-grupo-5-loan-default-byoc\"\n",
    "\n",
    "model_overview = ModelOverview(\n",
    "    model_name=\"Loan Default Binary Classifier (BYOC)\",\n",
    "    model_description=\"Binary classifier deployed with a custom BYOC container. Includes monitoring schedules.\",\n",
    "    problem_type=\"Binary Classification\",\n",
    "    algorithm_type=\"Custom container (sklearn/xgboost pipeline)\",\n",
    "    model_owner=\"AIDM Grupo 5\",\n",
    ")\n",
    "\n",
    "intended_uses = IntendedUses(\n",
    "    purpose_of_model=\"Predict probability of default for loan applications.\",\n",
    "    intended_uses=\"Decision support / risk scoring. Not a standalone decision maker.\",\n",
    "    factors_affecting_model_efficiency=\"Data drift and feature distribution shifts in applicant population.\",\n",
    "    risk_rating=RiskRatingEnum.MEDIUM,\n",
    "    explanations_for_risk_rating=\"Business-impacting decisions; requires monitoring and human oversight.\",\n",
    ")\n",
    "\n",
    "training_details = TrainingDetails(\n",
    "    objective_function=ObjectiveFunction(\n",
    "        function=Function(\n",
    "            function=ObjectiveFunctionEnum.MAXIMIZE,\n",
    "            facet=FacetEnum.AUC,\n",
    "            condition=\"Validation AUC (primary objective)\",\n",
    "        )\n",
    "    ),\n",
    "    training_observations=\"Training executed in SageMaker Training Jobs; Hyperparameter tuning executed via SageMaker HPO.\",\n",
    ")\n",
    "\n",
    "card = ModelCard(\n",
    "    name=model_card_name,\n",
    "    status=ModelCardStatusEnum.DRAFT,\n",
    "    model_overview=model_overview,\n",
    "    intended_uses=intended_uses,\n",
    "    training_details=training_details,\n",
    "    sagemaker_session=sm_session,\n",
    ")\n",
    "\n",
    "# cria ou atualiza\n",
    "try:\n",
    "    card.create()\n",
    "    print(\"Created Model Card:\", card.arn)\n",
    "except Exception as e:\n",
    "    # se já existir, faz update\n",
    "    print(\"Create failed (likely exists). Updating instead. Error:\", str(e)[:200])\n",
    "    card.update()\n",
    "    print(\"Updated Model Card:\", card.arn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b6ddc5-d186-4023-abe4-572ac16dcc74",
   "metadata": {},
   "source": [
    "## Garantir DataCapture no endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "346ee08d-4e96-4588-99ca-aa8f5f266e6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataCapture enabled. Destination: s3://i32419/ai-deployment-monitoring-grupo-5/datacapture\n"
     ]
    }
   ],
   "source": [
    "desc = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "cfg_name = desc[\"EndpointConfigName\"]\n",
    "cfg = sm_client.describe_endpoint_config(EndpointConfigName=cfg_name)\n",
    "\n",
    "if \"DataCaptureConfig\" not in cfg or not cfg[\"DataCaptureConfig\"].get(\"EnableCapture\", False):\n",
    "    raise RuntimeError(\n",
    "        f\"Endpoint {endpoint_name} does NOT have DataCapture enabled. \"\n",
    "        \"Create a new endpoint with DataCaptureConfig in the deployment notebook.\"\n",
    "    )\n",
    "\n",
    "print(\"DataCapture enabled. Destination:\",\n",
    "      cfg[\"DataCaptureConfig\"].get(\"DestinationS3Uri\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad769bf-e12c-4b1b-b98b-e957e6dfcc3b",
   "metadata": {},
   "source": [
    "## Preparar baseline dataset (a partir do CSV local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15536df8-a369-40a3-9bb4-52627b946625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline local saved: baseline_features.csv shape: (26329, 31)\n",
      "Baseline S3 URI: s3://i32419/ai-deployment-monitoring-grupo-5/monitoring/baseline/baseline_features.csv\n"
     ]
    }
   ],
   "source": [
    "train_path = \"data/train.csv\"\n",
    "val_path = \"data/validation.csv\"\n",
    "\n",
    "train_df = pd.read_csv(train_path)\n",
    "\n",
    "target_col = \"Status\"  # ajusta se necessário\n",
    "\n",
    "features_df = train_df.drop(columns=[target_col])\n",
    "baseline_local = \"baseline_features.csv\"\n",
    "\n",
    "# amostra para baseline para ser mais rápido\n",
    "features_df.sample(n=min(len(features_df), 2000), random_state=42).to_csv(baseline_local, index=False)\n",
    "\n",
    "print(\"baseline local saved:\", baseline_local, \"shape:\", features_df.shape)\n",
    "\n",
    "baseline_s3_uri = sm_session.upload_data(\n",
    "    baseline_local,\n",
    "    bucket=bucket,\n",
    "    key_prefix=f\"{prefix}/monitoring/baseline\"\n",
    ")\n",
    "print(\"Baseline S3 URI:\", baseline_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d96c65-86b4-4db6-a5f1-6934ad7b9178",
   "metadata": {},
   "source": [
    "## Criar baseline statistics/constraints + schedule (DefaultModelMonitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f639aa48-b259-4b23-9714-51792cd54e85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: None.\n",
      "INFO:sagemaker:Creating processing-job with name baseline-suggestion-job-2026-01-11-23-34-22-491\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "................\u001b[34m2026-01-11 23:37:04.982616: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:04.982651: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:06.430985: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:06.431021: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:06.431046: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-143-243.eu-west-1.compute.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:06.431302: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:07,745 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:eu-west-1:267567228900:processing-job/baseline-suggestion-job-2026-01-11-23-34-22-491', 'ProcessingJobName': 'baseline-suggestion-job-2026-01-11-23-34-22-491', 'Environment': {'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '468650794304.dkr.ecr.eu-west-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://i32419/ai-deployment-monitoring-grupo-5/monitoring/baseline/baseline_features.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://i32419/ai-deployment-monitoring-grupo-5/monitoring/baseline-output', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::267567228900:role/iseg-prd-sagemaker-role', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:07,745 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:07,745 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:07,745 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true, \"output_columns_position\": \"START\"}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:07,745 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:07,745 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:07,814 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:07,815 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:07,815 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.xlarge', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0', 'topology': None}\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:07,824 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:07,824 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:07,824 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,292 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.143.243\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/ya\u001b[0m\n",
      "\u001b[34mrn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_462\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,299 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,302 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-3e6cae45-ace8-4551-873c-f66ca474b937\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,783 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,795 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,796 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,799 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,803 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,803 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,803 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,804 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,835 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,846 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,846 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,850 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,853 INFO blockmanagement.BlockManager: The block deletion will start around 2026 Jan 11 23:37:08\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,854 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,854 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,855 INFO util.GSet: 2.0% max memory 3.1 GB = 62.8 MB\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,855 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,893 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,896 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,896 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,896 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,896 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,896 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,896 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,896 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,896 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,896 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,896 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,896 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,922 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,922 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,922 INFO util.GSet: 1.0% max memory 3.1 GB = 31.4 MB\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,922 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,925 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,925 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,925 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,925 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,929 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,933 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,933 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,933 INFO util.GSet: 0.25% max memory 3.1 GB = 7.8 MB\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,933 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,966 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,967 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,967 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,970 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,970 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,972 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,972 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,972 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 964.0 KB\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,972 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:08,994 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1404098942-10.0.143.243-1768174628987\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:09,008 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:09,016 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:09,097 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:09,111 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:09,114 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.143.243\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:09,126 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:11,199 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:11,199 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:13,288 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:13,288 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:15,394 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:15,394 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:17,491 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:17,496 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:19,583 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:19,584 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:29,596 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:31,184 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:31,592 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:31,633 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:31,643 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:32,222 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:32,249 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:32,249 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:32,250 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:32,250 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:32,278 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11393, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:32,293 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:32,295 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:32,344 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:32,345 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:32,345 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:32,345 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:32,346 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:32,770 INFO util.Utils: Successfully started service 'sparkDriver' on port 33775.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:32,805 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:32,839 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:32,857 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:32,858 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:32,903 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:32,933 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-4130374c-74be-4260-9a5e-23bb19bd9404\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:32,954 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:33,012 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:33,061 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.143.243:33775/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1768174652217\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:33,511 INFO client.RMProxy: Connecting to ResourceManager at /10.0.143.243:8032\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:34,216 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:34,217 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:34,223 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15536 MB per container)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:34,224 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:34,224 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:34,224 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:34,230 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:34,312 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:36,485 INFO yarn.Client: Uploading resource file:/tmp/spark-2f3fdbbd-a23a-4034-8163-2e8d4283d221/__spark_libs__7012558153721627439.zip -> hdfs://10.0.143.243/user/root/.sparkStaging/application_1768174634724_0001/__spark_libs__7012558153721627439.zip\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:37,649 INFO yarn.Client: Uploading resource file:/tmp/spark-2f3fdbbd-a23a-4034-8163-2e8d4283d221/__spark_conf__8963000485893071668.zip -> hdfs://10.0.143.243/user/root/.sparkStaging/application_1768174634724_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:38,100 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:38,100 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:38,101 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:38,101 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:38,101 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:38,128 INFO yarn.Client: Submitting application application_1768174634724_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:38,325 INFO impl.YarnClientImpl: Submitted application application_1768174634724_0001\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:39,329 INFO yarn.Client: Application report for application_1768174634724_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:39,334 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Sun Jan 11 23:37:38 +0000 2026] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1768174658229\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1768174634724_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:40,338 INFO yarn.Client: Application report for application_1768174634724_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:41,342 INFO yarn.Client: Application report for application_1768174634724_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:42,344 INFO yarn.Client: Application report for application_1768174634724_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:43,349 INFO yarn.Client: Application report for application_1768174634724_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:43,349 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.143.243\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1768174658229\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1768174634724_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:43,351 INFO cluster.YarnClientSchedulerBackend: Application application_1768174634724_0001 has started running.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:43,366 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38349.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:43,366 INFO netty.NettyBlockTransferService: Server created on 10.0.143.243:38349\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:43,369 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:43,377 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.143.243, 38349, None)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:43,381 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.143.243:38349 with 1458.6 MiB RAM, BlockManagerId(driver, 10.0.143.243, 38349, None)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:43,384 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.143.243, 38349, None)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:43,385 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.143.243, 38349, None)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:43,466 INFO util.log: Logging initialized @13695ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:43,582 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1768174634724_0001), /proxy/application_1768174634724_0001\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:44,873 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:47,836 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.143.243:37612) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:37:48,029 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:37531 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 37531, None)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:03,436 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:03,643 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:03,694 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:03,699 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:04,725 INFO datasources.InMemoryFileIndex: It took 39 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:04,886 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:05,214 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:05,217 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.143.243:38349 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:05,222 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:05,569 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:05,572 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:05,575 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 358915\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:05,632 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:05,649 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:05,650 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:05,650 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:05,651 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:05,658 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:05,684 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:05,686 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:05,687 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.143.243:38349 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:05,689 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:05,703 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:05,703 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:05,741 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4630 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:05,990 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:37531 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:06,764 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:37531 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:07,073 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1343 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:07,075 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:07,080 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.402 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:07,084 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:07,085 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:07,087 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.454033 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:07,271 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on algo-1:37531 in memory (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:07,288 INFO storage.BlockManagerInfo: Removed broadcast_0_piece0 on 10.0.143.243:38349 in memory (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:07,298 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:37531 in memory (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:07,304 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.143.243:38349 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,395 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,396 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,399 INFO datasources.FileSourceStrategy: Output Data Schema: struct<loan_limit: string, Gender: string, approv_in_adv: string, loan_type: string, loan_purpose: string ... 29 more fields>\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,443 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,620 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,633 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,634 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.143.243:38349 (size: 39.1 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,636 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,651 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,690 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,691 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,691 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,692 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,694 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,696 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,754 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 21.6 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,758 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 9.4 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,759 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.143.243:38349 (size: 9.4 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,760 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,760 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,760 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,764 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:09,828 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:37531 (size: 9.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:10,676 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:37531 (size: 39.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:10,889 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:37531 (size: 154.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:11,027 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 1265 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:11,027 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:11,028 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 1.329 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:11,028 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:11,029 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:11,029 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 1.339017 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:11,336 INFO codegen.CodeGenerator: Code generated in 224.630356 ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:11,967 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:11,972 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:11,972 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:11,973 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:11,976 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:11,980 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:12,009 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 118.2 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:12,025 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:12,026 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.143.243:38349 (size: 36.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:12,027 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:12,029 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:12,030 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:12,037 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:12,056 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:37531 (size: 36.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,363 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1327 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,364 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,366 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 1.382 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,366 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,367 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,367 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,368 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,470 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,472 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,472 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,472 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,472 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,474 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,487 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 171.0 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,488 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 47.3 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,489 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.143.243:38349 (size: 47.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,490 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,490 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,490 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,493 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,515 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:37531 (size: 47.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,566 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.143.243:37612\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,883 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 391 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,884 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,885 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.405 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,885 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,885 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,885 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.415682 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:13,922 INFO codegen.CodeGenerator: Code generated in 26.771591 ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:14,064 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:14,068 INFO scheduler.DAGScheduler: Registering RDD 27 (countByKey at ColumnProfiler.scala:592) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:14,069 INFO scheduler.DAGScheduler: Got job 4 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:14,069 INFO scheduler.DAGScheduler: Final stage: ResultStage 6 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:14,069 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:14,069 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 5)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:14,072 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[27] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:14,094 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 34.3 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:14,099 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 15.4 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:14,100 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.143.243:38349 (size: 15.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:14,100 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:14,101 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[27] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:14,101 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:14,103 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:14,129 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:37531 (size: 15.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,327 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 1225 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,327 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,328 INFO scheduler.DAGScheduler: ShuffleMapStage 5 (countByKey at ColumnProfiler.scala:592) finished in 1.255 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,329 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,329 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,329 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 6)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,329 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,335 INFO scheduler.DAGScheduler: Submitting ResultStage 6 (ShuffledRDD[28] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,338 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 5.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,340 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,340 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.143.243:38349 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,341 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,341 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (ShuffledRDD[28] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,342 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,344 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,359 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:37531 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,370 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.143.243:37612\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,404 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 61 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,405 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,406 INFO scheduler.DAGScheduler: ResultStage 6 (countByKey at ColumnProfiler.scala:592) finished in 0.069 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,406 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,406 INFO cluster.YarnScheduler: Killing all running tasks in stage 6: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,407 INFO scheduler.DAGScheduler: Job 4 finished: countByKey at ColumnProfiler.scala:592, took 1.342933 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,575 INFO scheduler.DAGScheduler: Registering RDD 33 (collect at AnalysisRunner.scala:326) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,575 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,576 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 7 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,576 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,577 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,578 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 7 (MapPartitionsRDD[33] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,584 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 87.2 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,586 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 28.5 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,587 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.143.243:38349 (size: 28.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,587 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,588 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[33] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,588 INFO cluster.YarnScheduler: Adding task set 7.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,589 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 7.0 (TID 6) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,605 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:37531 (size: 28.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,831 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 7.0 (TID 6) in 242 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,831 INFO cluster.YarnScheduler: Removed TaskSet 7.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,834 INFO scheduler.DAGScheduler: ShuffleMapStage 7 (collect at AnalysisRunner.scala:326) finished in 0.254 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,834 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,834 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,834 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,834 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,910 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,912 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,912 INFO scheduler.DAGScheduler: Final stage: ResultStage 9 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,913 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,913 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,914 INFO scheduler.DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[36] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,954 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 172.3 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,960 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 47.6 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,961 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.143.243:38349 (size: 47.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,961 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,962 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[36] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,962 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,964 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:15,984 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:37531 (size: 47.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,005 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.143.243:37612\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,127 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 163 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,129 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,130 INFO scheduler.DAGScheduler: ResultStage 9 (collect at AnalysisRunner.scala:326) finished in 0.215 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,130 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,130 INFO cluster.YarnScheduler: Killing all running tasks in stage 9: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,131 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.220956 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,346 INFO codegen.CodeGenerator: Code generated in 35.154582 ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,416 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,417 INFO scheduler.DAGScheduler: Got job 7 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,418 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,418 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,419 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,420 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[46] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,434 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 40.4 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,439 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 17.7 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,440 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.143.243:38349 (size: 17.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,440 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,441 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[46] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,441 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,443 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,464 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:37531 (size: 17.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,761 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 319 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,761 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,762 INFO scheduler.DAGScheduler: ResultStage 10 (treeReduce at KLLRunner.scala:107) finished in 0.340 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,762 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,762 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,762 INFO scheduler.DAGScheduler: Job 7 finished: treeReduce at KLLRunner.scala:107, took 0.346315 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,762 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.143.243:38349 in memory (size: 9.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,763 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:37531 in memory (size: 9.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,819 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.143.243:38349 in memory (size: 47.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,820 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:37531 in memory (size: 47.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,899 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:37531 in memory (size: 28.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,912 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.143.243:38349 in memory (size: 28.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,970 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:37531 in memory (size: 47.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:16,983 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.143.243:38349 in memory (size: 47.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,020 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:37531 in memory (size: 15.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,022 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.143.243:38349 in memory (size: 15.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,050 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.143.243:38349 in memory (size: 36.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,051 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:37531 in memory (size: 36.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,057 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:37531 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,060 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.143.243:38349 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,233 INFO codegen.CodeGenerator: Code generated in 63.580568 ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,239 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,239 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,239 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,239 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,240 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,241 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,245 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 47.1 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,247 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 17.8 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,247 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.143.243:38349 (size: 17.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,248 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,248 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,248 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,249 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,259 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:37531 (size: 17.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,329 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 80 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,329 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,330 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.088 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,330 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,330 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,330 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,330 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,430 INFO codegen.CodeGenerator: Code generated in 52.234451 ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,443 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,444 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,444 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,444 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,445 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,446 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,449 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 32.9 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,452 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 11.3 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,452 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.143.243:38349 (size: 11.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,453 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,454 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,454 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,455 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,466 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:37531 (size: 11.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,470 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.143.243:37612\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,542 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 87 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,543 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,544 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.095 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,544 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,544 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,545 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.101763 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,586 INFO codegen.CodeGenerator: Code generated in 30.463607 ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,653 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,655 INFO scheduler.DAGScheduler: Registering RDD 62 (countByKey at ColumnProfiler.scala:592) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,655 INFO scheduler.DAGScheduler: Got job 10 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,656 INFO scheduler.DAGScheduler: Final stage: ResultStage 15 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,656 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 14)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,656 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 14)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,660 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[62] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,666 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 34.3 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,668 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 15.4 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,669 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.143.243:38349 (size: 15.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,669 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,670 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[62] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,670 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,671 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,695 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:37531 (size: 15.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,772 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 101 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,772 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,773 INFO scheduler.DAGScheduler: ShuffleMapStage 14 (countByKey at ColumnProfiler.scala:592) finished in 0.112 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,777 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,777 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,777 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 15)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,778 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,778 INFO scheduler.DAGScheduler: Submitting ResultStage 15 (ShuffledRDD[63] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,780 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 5.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,782 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,783 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.143.243:38349 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,788 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,789 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (ShuffledRDD[63] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,789 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,790 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,804 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:37531 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,808 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.143.243:37612\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,840 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 49 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,840 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,841 INFO scheduler.DAGScheduler: ResultStage 15 (countByKey at ColumnProfiler.scala:592) finished in 0.062 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,841 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,842 INFO cluster.YarnScheduler: Killing all running tasks in stage 15: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:17,842 INFO scheduler.DAGScheduler: Job 10 finished: countByKey at ColumnProfiler.scala:592, took 0.188665 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,025 INFO scheduler.DAGScheduler: Registering RDD 68 (collect at AnalysisRunner.scala:326) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,025 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,025 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 16 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,026 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,026 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,027 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[68] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,030 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 87.2 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,032 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 28.4 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,032 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.143.243:38349 (size: 28.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,033 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,033 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[68] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,033 INFO cluster.YarnScheduler: Adding task set 16.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,034 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 16.0 (TID 13) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,045 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:37531 (size: 28.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,215 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 16.0 (TID 13) in 181 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,215 INFO cluster.YarnScheduler: Removed TaskSet 16.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,216 INFO scheduler.DAGScheduler: ShuffleMapStage 16 (collect at AnalysisRunner.scala:326) finished in 0.189 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,216 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,216 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,217 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,217 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,250 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,251 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,251 INFO scheduler.DAGScheduler: Final stage: ResultStage 18 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,251 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 17)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,251 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,251 INFO scheduler.DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[71] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,256 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 172.3 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,258 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 47.6 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,259 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.143.243:38349 (size: 47.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,259 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,260 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[71] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,260 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,261 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,269 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:37531 (size: 47.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,279 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.143.243:37612\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,383 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 122 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,383 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,384 INFO scheduler.DAGScheduler: ResultStage 18 (collect at AnalysisRunner.scala:326) finished in 0.132 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,386 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,386 INFO cluster.YarnScheduler: Killing all running tasks in stage 18: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,387 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.136518 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,479 INFO codegen.CodeGenerator: Code generated in 17.418117 ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,506 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,507 INFO scheduler.DAGScheduler: Got job 13 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,508 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,508 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,509 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,509 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[81] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,517 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 41.2 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,518 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 17.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,519 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.143.243:38349 (size: 17.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,519 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,520 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[81] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,520 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,521 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,533 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:37531 (size: 17.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,601 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 80 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,601 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,602 INFO scheduler.DAGScheduler: ResultStage 19 (treeReduce at KLLRunner.scala:107) finished in 0.091 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,605 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,606 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,606 INFO scheduler.DAGScheduler: Job 13 finished: treeReduce at KLLRunner.scala:107, took 0.099823 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,768 INFO codegen.CodeGenerator: Code generated in 49.62398 ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,775 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,775 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,776 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,776 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,776 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,776 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,780 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 56.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,782 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 19.8 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,782 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.143.243:38349 (size: 19.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,782 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,783 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,783 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,784 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,800 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:37531 (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,909 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 125 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,909 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,910 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.133 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,910 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,910 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,910 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:18,910 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,093 INFO codegen.CodeGenerator: Code generated in 94.351444 ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,122 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,125 INFO scheduler.DAGScheduler: Got job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,126 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,126 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,126 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,126 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,130 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 43.9 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,132 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 14.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,132 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.143.243:38349 (size: 14.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,133 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,133 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,134 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,135 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,159 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:37531 (size: 14.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,166 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.143.243:37612\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,241 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 106 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,241 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,242 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.113 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,242 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,242 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,242 INFO scheduler.DAGScheduler: Job 15 finished: collect at AnalysisRunner.scala:326, took 0.117458 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,355 INFO codegen.CodeGenerator: Code generated in 81.056731 ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,490 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:37531 in memory (size: 14.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,491 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.143.243:38349 in memory (size: 14.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,497 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,499 INFO scheduler.DAGScheduler: Registering RDD 97 (countByKey at ColumnProfiler.scala:592) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,499 INFO scheduler.DAGScheduler: Got job 16 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,499 INFO scheduler.DAGScheduler: Final stage: ResultStage 24 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,500 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,500 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 23)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,501 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[97] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,511 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 34.3 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,513 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 15.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,518 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.143.243:38349 (size: 15.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,519 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,519 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[97] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,519 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,521 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,535 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:37531 (size: 15.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,557 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.143.243:38349 in memory (size: 11.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,577 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 56 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,578 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,579 INFO scheduler.DAGScheduler: ShuffleMapStage 23 (countByKey at ColumnProfiler.scala:592) finished in 0.076 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,579 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,580 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,580 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 24)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,580 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,581 INFO scheduler.DAGScheduler: Submitting ResultStage 24 (ShuffledRDD[98] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,583 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 5.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,585 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,586 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:37531 in memory (size: 11.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,588 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.143.243:38349 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,590 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,599 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (ShuffledRDD[98] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,600 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,604 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,621 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:37531 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,626 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.143.243:38349 in memory (size: 17.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,628 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.143.243:37612\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,631 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:37531 in memory (size: 17.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,651 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 47 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,651 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,652 INFO scheduler.DAGScheduler: ResultStage 24 (countByKey at ColumnProfiler.scala:592) finished in 0.070 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,652 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,652 INFO cluster.YarnScheduler: Killing all running tasks in stage 24: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,652 INFO scheduler.DAGScheduler: Job 16 finished: countByKey at ColumnProfiler.scala:592, took 0.155134 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,675 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:37531 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,679 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.143.243:38349 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,685 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.143.243:38349 in memory (size: 15.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,689 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:37531 in memory (size: 15.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,706 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.143.243:38349 in memory (size: 17.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,706 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:37531 in memory (size: 17.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,741 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.143.243:38349 in memory (size: 17.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,754 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:37531 in memory (size: 17.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,764 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:37531 in memory (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,773 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.143.243:38349 in memory (size: 19.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,808 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.143.243:38349 in memory (size: 47.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,814 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:37531 in memory (size: 47.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,830 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.143.243:38349 in memory (size: 28.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,831 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:37531 in memory (size: 28.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,910 INFO scheduler.DAGScheduler: Registering RDD 103 (collect at AnalysisRunner.scala:326) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,911 INFO scheduler.DAGScheduler: Got map stage job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,911 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 25 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,911 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,912 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,912 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[103] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,917 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 87.2 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,921 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 28.5 KiB, free 1458.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,922 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.143.243:38349 (size: 28.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,922 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,923 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[103] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,923 INFO cluster.YarnScheduler: Adding task set 25.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,924 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 25.0 (TID 20) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:19,938 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:37531 (size: 28.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,120 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 25.0 (TID 20) in 196 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,121 INFO cluster.YarnScheduler: Removed TaskSet 25.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,121 INFO scheduler.DAGScheduler: ShuffleMapStage 25 (collect at AnalysisRunner.scala:326) finished in 0.208 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,122 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,122 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,122 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,123 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,179 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,180 INFO scheduler.DAGScheduler: Got job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,180 INFO scheduler.DAGScheduler: Final stage: ResultStage 27 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,180 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,180 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,181 INFO scheduler.DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[106] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,186 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 172.3 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,188 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 47.5 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,189 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.143.243:38349 (size: 47.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,190 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,190 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[106] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,190 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,192 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,205 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:37531 (size: 47.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,219 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.143.243:37612\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,362 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 170 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,362 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,363 INFO scheduler.DAGScheduler: ResultStage 27 (collect at AnalysisRunner.scala:326) finished in 0.182 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,363 INFO scheduler.DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,363 INFO cluster.YarnScheduler: Killing all running tasks in stage 27: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,364 INFO scheduler.DAGScheduler: Job 18 finished: collect at AnalysisRunner.scala:326, took 0.184239 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,480 INFO codegen.CodeGenerator: Code generated in 22.954866 ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,533 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,535 INFO scheduler.DAGScheduler: Got job 19 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,535 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,535 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,536 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,537 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[116] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,548 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 39.7 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,552 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,552 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.143.243:38349 (size: 17.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,553 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,554 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (MapPartitionsRDD[116] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,554 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,555 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,565 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:37531 (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,647 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 91 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,655 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,656 INFO scheduler.DAGScheduler: ResultStage 28 (treeReduce at KLLRunner.scala:107) finished in 0.118 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,657 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,657 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,658 INFO scheduler.DAGScheduler: Job 19 finished: treeReduce at KLLRunner.scala:107, took 0.123673 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,769 INFO codegen.CodeGenerator: Code generated in 41.835167 ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,776 INFO scheduler.DAGScheduler: Registering RDD 121 (collect at AnalysisRunner.scala:326) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,779 INFO scheduler.DAGScheduler: Got map stage job 20 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,779 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,779 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,780 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,780 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,785 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 36.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,787 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 14.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,787 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.143.243:38349 (size: 14.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,787 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,788 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,788 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,789 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,799 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:37531 (size: 14.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,849 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 60 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,849 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,850 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326) finished in 0.068 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,851 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,853 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,853 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,853 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,922 INFO codegen.CodeGenerator: Code generated in 46.049828 ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,937 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,939 INFO scheduler.DAGScheduler: Got job 21 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,939 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,939 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,939 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,939 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,941 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 21.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,943 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,943 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.143.243:38349 (size: 8.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,943 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,944 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,944 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,945 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 24) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,956 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:37531 (size: 8.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:20,960 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.143.243:37612\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,017 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 24) in 72 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,017 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,018 INFO scheduler.DAGScheduler: ResultStage 31 (collect at AnalysisRunner.scala:326) finished in 0.078 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,019 INFO scheduler.DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,020 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,020 INFO scheduler.DAGScheduler: Job 21 finished: collect at AnalysisRunner.scala:326, took 0.082692 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,049 INFO codegen.CodeGenerator: Code generated in 21.263229 ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,114 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,116 INFO scheduler.DAGScheduler: Registering RDD 132 (countByKey at ColumnProfiler.scala:592) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,118 INFO scheduler.DAGScheduler: Got job 22 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,118 INFO scheduler.DAGScheduler: Final stage: ResultStage 33 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,119 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 32)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,119 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 32)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,120 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 32 (MapPartitionsRDD[132] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,130 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 34.3 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,131 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 15.4 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,132 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.143.243:38349 (size: 15.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,132 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,132 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 32 (MapPartitionsRDD[132] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,133 INFO cluster.YarnScheduler: Adding task set 32.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,134 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 25) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,144 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:37531 (size: 15.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,178 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 25) in 44 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,178 INFO cluster.YarnScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,178 INFO scheduler.DAGScheduler: ShuffleMapStage 32 (countByKey at ColumnProfiler.scala:592) finished in 0.054 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,179 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,179 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,179 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 33)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,179 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,180 INFO scheduler.DAGScheduler: Submitting ResultStage 33 (ShuffledRDD[133] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,181 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 5.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,183 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,183 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.143.243:38349 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,184 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,184 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 33 (ShuffledRDD[133] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,184 INFO cluster.YarnScheduler: Adding task set 33.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,186 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 26) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,195 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-1:37531 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,201 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.0.143.243:37612\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,210 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 26) in 25 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,210 INFO cluster.YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,211 INFO scheduler.DAGScheduler: ResultStage 33 (countByKey at ColumnProfiler.scala:592) finished in 0.031 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,211 INFO scheduler.DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,211 INFO cluster.YarnScheduler: Killing all running tasks in stage 33: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,213 INFO scheduler.DAGScheduler: Job 22 finished: countByKey at ColumnProfiler.scala:592, took 0.097961 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,375 INFO scheduler.DAGScheduler: Registering RDD 138 (collect at AnalysisRunner.scala:326) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,375 INFO scheduler.DAGScheduler: Got map stage job 23 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,376 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 34 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,376 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,377 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,377 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 34 (MapPartitionsRDD[138] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,381 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 87.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,383 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 28.5 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,384 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.143.243:38349 (size: 28.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,384 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,385 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 34 (MapPartitionsRDD[138] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,385 INFO cluster.YarnScheduler: Adding task set 34.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,386 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 34.0 (TID 27) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,399 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-1:37531 (size: 28.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,521 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 34.0 (TID 27) in 135 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,521 INFO cluster.YarnScheduler: Removed TaskSet 34.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,522 INFO scheduler.DAGScheduler: ShuffleMapStage 34 (collect at AnalysisRunner.scala:326) finished in 0.144 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,522 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,523 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,523 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,523 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,578 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,579 INFO scheduler.DAGScheduler: Got job 24 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,579 INFO scheduler.DAGScheduler: Final stage: ResultStage 36 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,579 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 35)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,580 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,580 INFO scheduler.DAGScheduler: Submitting ResultStage 36 (MapPartitionsRDD[141] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,586 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 172.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,589 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 47.5 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,596 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.143.243:38349 (size: 47.5 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,596 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,597 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 36 (MapPartitionsRDD[141] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,597 INFO cluster.YarnScheduler: Adding task set 36.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,599 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 28) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,609 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on algo-1:37531 (size: 47.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,623 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.0.143.243:37612\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,720 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 28) in 121 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,720 INFO cluster.YarnScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,721 INFO scheduler.DAGScheduler: ResultStage 36 (collect at AnalysisRunner.scala:326) finished in 0.140 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,721 INFO scheduler.DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,721 INFO cluster.YarnScheduler: Killing all running tasks in stage 36: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,722 INFO scheduler.DAGScheduler: Job 24 finished: collect at AnalysisRunner.scala:326, took 0.143316 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,812 INFO codegen.CodeGenerator: Code generated in 12.305624 ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,844 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,844 INFO scheduler.DAGScheduler: Got job 25 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,845 INFO scheduler.DAGScheduler: Final stage: ResultStage 37 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,845 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,845 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,846 INFO scheduler.DAGScheduler: Submitting ResultStage 37 (MapPartitionsRDD[151] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,853 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 40.5 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,854 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,855 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.0.143.243:38349 (size: 17.6 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,856 INFO spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,856 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (MapPartitionsRDD[151] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,856 INFO cluster.YarnScheduler: Adding task set 37.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,858 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 37.0 (TID 29) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,868 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on algo-1:37531 (size: 17.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,912 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 37.0 (TID 29) in 54 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,912 INFO cluster.YarnScheduler: Removed TaskSet 37.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,913 INFO scheduler.DAGScheduler: ResultStage 37 (treeReduce at KLLRunner.scala:107) finished in 0.066 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,915 INFO scheduler.DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,916 INFO cluster.YarnScheduler: Killing all running tasks in stage 37: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:21,916 INFO scheduler.DAGScheduler: Job 25 finished: treeReduce at KLLRunner.scala:107, took 0.072091 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,018 INFO codegen.CodeGenerator: Code generated in 28.046045 ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,025 INFO scheduler.DAGScheduler: Registering RDD 156 (collect at AnalysisRunner.scala:326) as input to shuffle 12\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,025 INFO scheduler.DAGScheduler: Got map stage job 26 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,025 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 38 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,025 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,026 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,026 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[156] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,029 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 46.6 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,030 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 17.6 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,030 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.0.143.243:38349 (size: 17.6 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,031 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,031 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[156] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,032 INFO cluster.YarnScheduler: Adding task set 38.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,033 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 30) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,043 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on algo-1:37531 (size: 17.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,084 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 30) in 51 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,084 INFO cluster.YarnScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,085 INFO scheduler.DAGScheduler: ShuffleMapStage 38 (collect at AnalysisRunner.scala:326) finished in 0.058 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,085 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,086 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,086 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,086 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,148 INFO codegen.CodeGenerator: Code generated in 40.744898 ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,167 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,168 INFO scheduler.DAGScheduler: Got job 27 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,169 INFO scheduler.DAGScheduler: Final stage: ResultStage 40 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,169 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,169 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,170 INFO scheduler.DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[159] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,172 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 32.6 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,175 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 11.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,176 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.0.143.243:38349 (size: 11.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,177 INFO spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,177 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[159] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,177 INFO cluster.YarnScheduler: Adding task set 40.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,178 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 31) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,189 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on algo-1:37531 (size: 11.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,193 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 10.0.143.243:37612\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,241 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 31) in 63 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,241 INFO cluster.YarnScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,242 INFO scheduler.DAGScheduler: ResultStage 40 (collect at AnalysisRunner.scala:326) finished in 0.071 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,243 INFO scheduler.DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,243 INFO cluster.YarnScheduler: Killing all running tasks in stage 40: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,244 INFO scheduler.DAGScheduler: Job 27 finished: collect at AnalysisRunner.scala:326, took 0.076396 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,301 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,302 INFO scheduler.DAGScheduler: Registering RDD 167 (countByKey at ColumnProfiler.scala:592) as input to shuffle 13\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,303 INFO scheduler.DAGScheduler: Got job 28 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,303 INFO scheduler.DAGScheduler: Final stage: ResultStage 42 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,303 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 41)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,303 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 41)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,306 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 41 (MapPartitionsRDD[167] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,315 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 34.3 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,321 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 15.4 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,321 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.0.143.243:38349 (size: 15.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,322 INFO spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,323 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[167] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,323 INFO cluster.YarnScheduler: Adding task set 41.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,328 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 41.0 (TID 32) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,338 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on algo-1:37531 (size: 15.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,375 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 41.0 (TID 32) in 47 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,375 INFO cluster.YarnScheduler: Removed TaskSet 41.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,376 INFO scheduler.DAGScheduler: ShuffleMapStage 41 (countByKey at ColumnProfiler.scala:592) finished in 0.070 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,376 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,376 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,376 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 42)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,376 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,376 INFO scheduler.DAGScheduler: Submitting ResultStage 42 (ShuffledRDD[168] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,380 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 5.1 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,384 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,385 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.0.143.243:38349 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,386 INFO spark.SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,386 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (ShuffledRDD[168] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,386 INFO cluster.YarnScheduler: Adding task set 42.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,387 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 42.0 (TID 33) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,404 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on algo-1:37531 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,409 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to 10.0.143.243:37612\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,422 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 42.0 (TID 33) in 35 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,422 INFO cluster.YarnScheduler: Removed TaskSet 42.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,422 INFO scheduler.DAGScheduler: ResultStage 42 (countByKey at ColumnProfiler.scala:592) finished in 0.043 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,422 INFO scheduler.DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,422 INFO cluster.YarnScheduler: Killing all running tasks in stage 42: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,423 INFO scheduler.DAGScheduler: Job 28 finished: countByKey at ColumnProfiler.scala:592, took 0.121854 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,521 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-1:37531 in memory (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,522 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.0.143.243:38349 in memory (size: 17.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,531 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.0.143.243:38349 in memory (size: 14.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,542 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on algo-1:37531 in memory (size: 14.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,544 INFO scheduler.DAGScheduler: Registering RDD 173 (collect at AnalysisRunner.scala:326) as input to shuffle 14\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,544 INFO scheduler.DAGScheduler: Got map stage job 29 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,545 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 43 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,545 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,546 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,546 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 43 (MapPartitionsRDD[173] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,551 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 87.2 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,553 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 28.5 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,553 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.0.143.243:38349 (size: 28.5 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,554 INFO spark.SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,554 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 43 (MapPartitionsRDD[173] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,555 INFO cluster.YarnScheduler: Adding task set 43.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,556 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 43.0 (TID 34) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,591 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on algo-1:37531 (size: 28.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,600 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on 10.0.143.243:38349 in memory (size: 15.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,616 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on algo-1:37531 in memory (size: 15.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,631 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:37531 in memory (size: 47.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,636 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.0.143.243:38349 in memory (size: 47.5 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,649 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on algo-1:37531 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,653 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.0.143.243:38349 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,681 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.143.243:38349 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,689 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:37531 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,700 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on algo-1:37531 in memory (size: 17.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,704 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on 10.0.143.243:38349 in memory (size: 17.6 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,724 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on algo-1:37531 in memory (size: 8.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,731 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.0.143.243:38349 in memory (size: 8.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,735 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.143.243:38349 in memory (size: 28.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,743 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:37531 in memory (size: 28.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,750 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on 10.0.143.243:38349 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,751 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on algo-1:37531 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,760 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on 10.0.143.243:38349 in memory (size: 11.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,760 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on algo-1:37531 in memory (size: 11.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,762 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 10.0.143.243:38349 in memory (size: 28.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,763 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on algo-1:37531 in memory (size: 28.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,767 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.143.243:38349 in memory (size: 15.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,768 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:37531 in memory (size: 15.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,771 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.0.143.243:38349 in memory (size: 17.6 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,772 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on algo-1:37531 in memory (size: 17.6 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,783 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.0.143.243:38349 in memory (size: 47.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,784 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on algo-1:37531 in memory (size: 47.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,786 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.0.143.243:38349 in memory (size: 15.4 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,787 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-1:37531 in memory (size: 15.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,829 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 43.0 (TID 34) in 273 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,829 INFO cluster.YarnScheduler: Removed TaskSet 43.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,830 INFO scheduler.DAGScheduler: ShuffleMapStage 43 (collect at AnalysisRunner.scala:326) finished in 0.283 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,831 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,831 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,831 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,831 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,880 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,883 INFO scheduler.DAGScheduler: Got job 30 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,883 INFO scheduler.DAGScheduler: Final stage: ResultStage 45 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,884 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 44)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,885 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,886 INFO scheduler.DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[176] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,894 INFO memory.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 172.1 KiB, free 1457.9 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,895 INFO memory.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 47.5 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,896 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.0.143.243:38349 (size: 47.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,896 INFO spark.SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,897 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[176] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,897 INFO cluster.YarnScheduler: Adding task set 45.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,898 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 45.0 (TID 35) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,907 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on algo-1:37531 (size: 47.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:22,924 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to 10.0.143.243:37612\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,003 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 45.0 (TID 35) in 105 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,003 INFO cluster.YarnScheduler: Removed TaskSet 45.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,004 INFO scheduler.DAGScheduler: ResultStage 45 (collect at AnalysisRunner.scala:326) finished in 0.116 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,004 INFO scheduler.DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,004 INFO cluster.YarnScheduler: Killing all running tasks in stage 45: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,004 INFO scheduler.DAGScheduler: Job 30 finished: collect at AnalysisRunner.scala:326, took 0.123693 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,062 INFO codegen.CodeGenerator: Code generated in 15.363521 ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,092 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,093 INFO scheduler.DAGScheduler: Got job 31 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,093 INFO scheduler.DAGScheduler: Final stage: ResultStage 46 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,093 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,093 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,094 INFO scheduler.DAGScheduler: Submitting ResultStage 46 (MapPartitionsRDD[186] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,103 INFO memory.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 39.7 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,105 INFO memory.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 17.3 KiB, free 1457.8 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,106 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.0.143.243:38349 (size: 17.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,106 INFO spark.SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,107 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[186] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,107 INFO cluster.YarnScheduler: Adding task set 46.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,108 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 46.0 (TID 36) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,118 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on algo-1:37531 (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,174 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 46.0 (TID 36) in 66 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,174 INFO cluster.YarnScheduler: Removed TaskSet 46.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,175 INFO scheduler.DAGScheduler: ResultStage 46 (treeReduce at KLLRunner.scala:107) finished in 0.081 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,175 INFO scheduler.DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,175 INFO cluster.YarnScheduler: Killing all running tasks in stage 46: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,175 INFO scheduler.DAGScheduler: Job 31 finished: treeReduce at KLLRunner.scala:107, took 0.082937 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,256 INFO scheduler.DAGScheduler: Registering RDD 191 (collect at AnalysisRunner.scala:326) as input to shuffle 15\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,256 INFO scheduler.DAGScheduler: Got map stage job 32 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,256 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 47 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,257 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,257 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,258 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 47 (MapPartitionsRDD[191] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,260 INFO memory.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 36.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,261 INFO memory.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 14.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,261 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.0.143.243:38349 (size: 14.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,262 INFO spark.SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,262 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 47 (MapPartitionsRDD[191] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,262 INFO cluster.YarnScheduler: Adding task set 47.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,263 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 47.0 (TID 37) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,274 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on algo-1:37531 (size: 14.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,288 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 47.0 (TID 37) in 25 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,288 INFO cluster.YarnScheduler: Removed TaskSet 47.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,288 INFO scheduler.DAGScheduler: ShuffleMapStage 47 (collect at AnalysisRunner.scala:326) finished in 0.030 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,288 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,288 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,288 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,289 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,311 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,312 INFO scheduler.DAGScheduler: Got job 33 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,312 INFO scheduler.DAGScheduler: Final stage: ResultStage 49 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,312 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 48)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,312 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,312 INFO scheduler.DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[194] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,314 INFO memory.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 21.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,315 INFO memory.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,315 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.0.143.243:38349 (size: 8.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,316 INFO spark.SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,316 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[194] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,316 INFO cluster.YarnScheduler: Adding task set 49.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,317 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 49.0 (TID 38) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,326 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on algo-1:37531 (size: 8.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,330 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 15 to 10.0.143.243:37612\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,338 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 49.0 (TID 38) in 21 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,338 INFO cluster.YarnScheduler: Removed TaskSet 49.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,339 INFO scheduler.DAGScheduler: ResultStage 49 (collect at AnalysisRunner.scala:326) finished in 0.026 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,339 INFO scheduler.DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,339 INFO cluster.YarnScheduler: Killing all running tasks in stage 49: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,340 INFO scheduler.DAGScheduler: Job 33 finished: collect at AnalysisRunner.scala:326, took 0.028263 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,383 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,384 INFO scheduler.DAGScheduler: Registering RDD 202 (countByKey at ColumnProfiler.scala:592) as input to shuffle 16\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,385 INFO scheduler.DAGScheduler: Got job 34 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,385 INFO scheduler.DAGScheduler: Final stage: ResultStage 51 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,385 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 50)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,385 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 50)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,386 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 50 (MapPartitionsRDD[202] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,392 INFO memory.MemoryStore: Block broadcast_41 stored as values in memory (estimated size 34.3 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,394 INFO memory.MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 15.4 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,394 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on 10.0.143.243:38349 (size: 15.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,395 INFO spark.SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,401 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 50 (MapPartitionsRDD[202] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,402 INFO cluster.YarnScheduler: Adding task set 50.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,403 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 50.0 (TID 39) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,411 INFO storage.BlockManagerInfo: Added broadcast_41_piece0 in memory on algo-1:37531 (size: 15.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,441 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 50.0 (TID 39) in 38 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,441 INFO cluster.YarnScheduler: Removed TaskSet 50.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,442 INFO scheduler.DAGScheduler: ShuffleMapStage 50 (countByKey at ColumnProfiler.scala:592) finished in 0.056 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,442 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,443 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,443 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 51)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,443 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,443 INFO scheduler.DAGScheduler: Submitting ResultStage 51 (ShuffledRDD[203] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,445 INFO memory.MemoryStore: Block broadcast_42 stored as values in memory (estimated size 5.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,451 INFO memory.MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,451 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on 10.0.143.243:38349 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,454 INFO spark.SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,454 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 51 (ShuffledRDD[203] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,454 INFO cluster.YarnScheduler: Adding task set 51.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,456 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 51.0 (TID 40) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,462 INFO storage.BlockManagerInfo: Added broadcast_42_piece0 in memory on algo-1:37531 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,465 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to 10.0.143.243:37612\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,472 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 51.0 (TID 40) in 17 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,472 INFO cluster.YarnScheduler: Removed TaskSet 51.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,472 INFO scheduler.DAGScheduler: ResultStage 51 (countByKey at ColumnProfiler.scala:592) finished in 0.028 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,473 INFO scheduler.DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,473 INFO cluster.YarnScheduler: Killing all running tasks in stage 51: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,473 INFO scheduler.DAGScheduler: Job 34 finished: countByKey at ColumnProfiler.scala:592, took 0.089375 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,611 INFO scheduler.DAGScheduler: Registering RDD 208 (collect at AnalysisRunner.scala:326) as input to shuffle 17\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,612 INFO scheduler.DAGScheduler: Got map stage job 35 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,612 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 52 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,612 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,612 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,613 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 52 (MapPartitionsRDD[208] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,615 INFO memory.MemoryStore: Block broadcast_43 stored as values in memory (estimated size 46.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,616 INFO memory.MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 19.0 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,617 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on 10.0.143.243:38349 (size: 19.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,617 INFO spark.SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,617 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 52 (MapPartitionsRDD[208] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,617 INFO cluster.YarnScheduler: Adding task set 52.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,619 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 52.0 (TID 41) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,626 INFO storage.BlockManagerInfo: Added broadcast_43_piece0 in memory on algo-1:37531 (size: 19.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,693 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 52.0 (TID 41) in 75 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,693 INFO cluster.YarnScheduler: Removed TaskSet 52.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,694 INFO scheduler.DAGScheduler: ShuffleMapStage 52 (collect at AnalysisRunner.scala:326) finished in 0.080 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,694 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,694 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,694 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,694 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,717 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,718 INFO scheduler.DAGScheduler: Got job 36 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,719 INFO scheduler.DAGScheduler: Final stage: ResultStage 54 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,719 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 53)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,719 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,719 INFO scheduler.DAGScheduler: Submitting ResultStage 54 (MapPartitionsRDD[211] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,725 INFO memory.MemoryStore: Block broadcast_44 stored as values in memory (estimated size 66.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,727 INFO memory.MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 24.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,727 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on 10.0.143.243:38349 (size: 24.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,728 INFO spark.SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,728 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 54 (MapPartitionsRDD[211] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,728 INFO cluster.YarnScheduler: Adding task set 54.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,731 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 54.0 (TID 42) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,744 INFO storage.BlockManagerInfo: Added broadcast_44_piece0 in memory on algo-1:37531 (size: 24.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,751 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 17 to 10.0.143.243:37612\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,798 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 54.0 (TID 42) in 67 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,798 INFO cluster.YarnScheduler: Removed TaskSet 54.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,798 INFO scheduler.DAGScheduler: ResultStage 54 (collect at AnalysisRunner.scala:326) finished in 0.076 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,799 INFO scheduler.DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,800 INFO cluster.YarnScheduler: Killing all running tasks in stage 54: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,800 INFO scheduler.DAGScheduler: Job 36 finished: collect at AnalysisRunner.scala:326, took 0.082699 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,809 INFO codegen.CodeGenerator: Code generated in 7.13871 ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,864 INFO codegen.CodeGenerator: Code generated in 13.337255 ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,886 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,886 INFO scheduler.DAGScheduler: Got job 37 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,886 INFO scheduler.DAGScheduler: Final stage: ResultStage 55 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,886 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,887 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,887 INFO scheduler.DAGScheduler: Submitting ResultStage 55 (MapPartitionsRDD[221] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,893 INFO memory.MemoryStore: Block broadcast_45 stored as values in memory (estimated size 37.6 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,894 INFO memory.MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 16.5 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,894 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on 10.0.143.243:38349 (size: 16.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,894 INFO spark.SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,895 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 55 (MapPartitionsRDD[221] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,895 INFO cluster.YarnScheduler: Adding task set 55.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,896 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 55.0 (TID 43) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4958 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,905 INFO storage.BlockManagerInfo: Added broadcast_45_piece0 in memory on algo-1:37531 (size: 16.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,944 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 55.0 (TID 43) in 48 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,944 INFO cluster.YarnScheduler: Removed TaskSet 55.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,944 INFO scheduler.DAGScheduler: ResultStage 55 (treeReduce at KLLRunner.scala:107) finished in 0.056 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,945 INFO scheduler.DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,945 INFO cluster.YarnScheduler: Killing all running tasks in stage 55: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:23,946 INFO scheduler.DAGScheduler: Job 37 finished: treeReduce at KLLRunner.scala:107, took 0.059794 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,023 INFO scheduler.DAGScheduler: Registering RDD 226 (collect at AnalysisRunner.scala:326) as input to shuffle 18\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,023 INFO scheduler.DAGScheduler: Got map stage job 38 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,023 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 56 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,023 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,023 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,024 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 56 (MapPartitionsRDD[226] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,028 INFO memory.MemoryStore: Block broadcast_46 stored as values in memory (estimated size 36.5 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,029 INFO memory.MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 14.9 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,030 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on 10.0.143.243:38349 (size: 14.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,030 INFO spark.SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,030 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 56 (MapPartitionsRDD[226] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,030 INFO cluster.YarnScheduler: Adding task set 56.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,031 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 56.0 (TID 44) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,038 INFO storage.BlockManagerInfo: Added broadcast_46_piece0 in memory on algo-1:37531 (size: 14.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,047 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 56.0 (TID 44) in 16 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,047 INFO cluster.YarnScheduler: Removed TaskSet 56.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,048 INFO scheduler.DAGScheduler: ShuffleMapStage 56 (collect at AnalysisRunner.scala:326) finished in 0.024 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,048 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,048 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,048 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,048 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,070 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,071 INFO scheduler.DAGScheduler: Got job 39 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,071 INFO scheduler.DAGScheduler: Final stage: ResultStage 58 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,071 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 57)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,071 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,072 INFO scheduler.DAGScheduler: Submitting ResultStage 58 (MapPartitionsRDD[229] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,073 INFO memory.MemoryStore: Block broadcast_47 stored as values in memory (estimated size 21.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,074 INFO memory.MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 8.3 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,075 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on 10.0.143.243:38349 (size: 8.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,075 INFO spark.SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,075 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 58 (MapPartitionsRDD[229] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,075 INFO cluster.YarnScheduler: Adding task set 58.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,076 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 58.0 (TID 45) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,083 INFO storage.BlockManagerInfo: Added broadcast_47_piece0 in memory on algo-1:37531 (size: 8.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,086 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to 10.0.143.243:37612\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,090 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 58.0 (TID 45) in 14 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,091 INFO cluster.YarnScheduler: Removed TaskSet 58.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,091 INFO scheduler.DAGScheduler: ResultStage 58 (collect at AnalysisRunner.scala:326) finished in 0.019 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,091 INFO scheduler.DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,091 INFO cluster.YarnScheduler: Killing all running tasks in stage 58: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,091 INFO scheduler.DAGScheduler: Job 39 finished: collect at AnalysisRunner.scala:326, took 0.020711 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,124 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,125 INFO scheduler.DAGScheduler: Registering RDD 237 (countByKey at ColumnProfiler.scala:592) as input to shuffle 19\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,125 INFO scheduler.DAGScheduler: Got job 40 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,125 INFO scheduler.DAGScheduler: Final stage: ResultStage 60 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,125 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 59)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,125 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 59)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,126 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 59 (MapPartitionsRDD[237] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,129 INFO memory.MemoryStore: Block broadcast_48 stored as values in memory (estimated size 33.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,131 INFO memory.MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 15.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,131 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on 10.0.143.243:38349 (size: 15.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,131 INFO spark.SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,132 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 59 (MapPartitionsRDD[237] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,132 INFO cluster.YarnScheduler: Adding task set 59.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,133 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 59.0 (TID 46) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,139 INFO storage.BlockManagerInfo: Added broadcast_48_piece0 in memory on algo-1:37531 (size: 15.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,163 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 59.0 (TID 46) in 30 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,164 INFO cluster.YarnScheduler: Removed TaskSet 59.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,164 INFO scheduler.DAGScheduler: ShuffleMapStage 59 (countByKey at ColumnProfiler.scala:592) finished in 0.038 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,164 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,164 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,164 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 60)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,164 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,164 INFO scheduler.DAGScheduler: Submitting ResultStage 60 (ShuffledRDD[238] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,165 INFO memory.MemoryStore: Block broadcast_49 stored as values in memory (estimated size 5.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,166 INFO memory.MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,167 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on 10.0.143.243:38349 (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,167 INFO spark.SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,167 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 60 (ShuffledRDD[238] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,167 INFO cluster.YarnScheduler: Adding task set 60.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,168 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 60.0 (TID 47) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,174 INFO storage.BlockManagerInfo: Added broadcast_49_piece0 in memory on algo-1:37531 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,176 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 19 to 10.0.143.243:37612\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,183 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 60.0 (TID 47) in 15 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,183 INFO cluster.YarnScheduler: Removed TaskSet 60.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,183 INFO scheduler.DAGScheduler: ResultStage 60 (countByKey at ColumnProfiler.scala:592) finished in 0.018 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,184 INFO scheduler.DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,184 INFO cluster.YarnScheduler: Killing all running tasks in stage 60: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,184 INFO scheduler.DAGScheduler: Job 40 finished: countByKey at ColumnProfiler.scala:592, took 0.059831 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,353 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,382 INFO codegen.CodeGenerator: Code generated in 8.459529 ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,386 INFO scheduler.DAGScheduler: Registering RDD 243 (count at StatsGenerator.scala:66) as input to shuffle 20\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,387 INFO scheduler.DAGScheduler: Got map stage job 41 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,387 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 61 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,387 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,387 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,387 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 61 (MapPartitionsRDD[243] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,390 INFO memory.MemoryStore: Block broadcast_50 stored as values in memory (estimated size 26.4 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,391 INFO memory.MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 11.5 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,391 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on 10.0.143.243:38349 (size: 11.5 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,392 INFO spark.SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,392 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 61 (MapPartitionsRDD[243] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,392 INFO cluster.YarnScheduler: Adding task set 61.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,393 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 61.0 (TID 48) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4947 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,400 INFO storage.BlockManagerInfo: Added broadcast_50_piece0 in memory on algo-1:37531 (size: 11.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,426 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 61.0 (TID 48) in 33 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,426 INFO cluster.YarnScheduler: Removed TaskSet 61.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,426 INFO scheduler.DAGScheduler: ShuffleMapStage 61 (count at StatsGenerator.scala:66) finished in 0.038 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,427 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,427 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,427 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,427 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,440 INFO codegen.CodeGenerator: Code generated in 6.93469 ms\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,447 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,447 INFO scheduler.DAGScheduler: Got job 42 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,447 INFO scheduler.DAGScheduler: Final stage: ResultStage 63 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,447 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 62)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,448 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,448 INFO scheduler.DAGScheduler: Submitting ResultStage 63 (MapPartitionsRDD[246] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,449 INFO memory.MemoryStore: Block broadcast_51 stored as values in memory (estimated size 11.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,450 INFO memory.MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,450 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on 10.0.143.243:38349 (size: 5.5 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,450 INFO spark.SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,451 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 63 (MapPartitionsRDD[246] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,451 INFO cluster.YarnScheduler: Adding task set 63.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,452 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 63.0 (TID 49) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,458 INFO storage.BlockManagerInfo: Added broadcast_51_piece0 in memory on algo-1:37531 (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,461 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 20 to 10.0.143.243:37612\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,472 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 63.0 (TID 49) in 21 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,472 INFO cluster.YarnScheduler: Removed TaskSet 63.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,472 INFO scheduler.DAGScheduler: ResultStage 63 (count at StatsGenerator.scala:66) finished in 0.024 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,472 INFO scheduler.DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,472 INFO cluster.YarnScheduler: Killing all running tasks in stage 63: Stage finished\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,473 INFO scheduler.DAGScheduler: Job 42 finished: count at StatsGenerator.scala:66, took 0.025779 s\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,597 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on 10.0.143.243:38349 in memory (size: 14.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,598 INFO storage.BlockManagerInfo: Removed broadcast_46_piece0 on algo-1:37531 in memory (size: 14.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,605 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on 10.0.143.243:38349 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,609 INFO storage.BlockManagerInfo: Removed broadcast_49_piece0 on algo-1:37531 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,612 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on 10.0.143.243:38349 in memory (size: 14.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,613 INFO storage.BlockManagerInfo: Removed broadcast_39_piece0 on algo-1:37531 in memory (size: 14.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,617 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on 10.0.143.243:38349 in memory (size: 19.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,618 INFO storage.BlockManagerInfo: Removed broadcast_43_piece0 on algo-1:37531 in memory (size: 19.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,621 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on 10.0.143.243:38349 in memory (size: 15.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,622 INFO storage.BlockManagerInfo: Removed broadcast_48_piece0 on algo-1:37531 in memory (size: 15.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,628 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on 10.0.143.243:38349 in memory (size: 15.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,629 INFO storage.BlockManagerInfo: Removed broadcast_41_piece0 on algo-1:37531 in memory (size: 15.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,633 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on 10.0.143.243:38349 in memory (size: 5.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,633 INFO storage.BlockManagerInfo: Removed broadcast_51_piece0 on algo-1:37531 in memory (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,636 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on 10.0.143.243:38349 in memory (size: 17.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,636 INFO storage.BlockManagerInfo: Removed broadcast_38_piece0 on algo-1:37531 in memory (size: 17.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,645 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on 10.0.143.243:38349 in memory (size: 24.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,649 INFO storage.BlockManagerInfo: Removed broadcast_44_piece0 on algo-1:37531 in memory (size: 24.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,651 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on 10.0.143.243:38349 in memory (size: 16.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,653 INFO storage.BlockManagerInfo: Removed broadcast_45_piece0 on algo-1:37531 in memory (size: 16.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,666 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on 10.0.143.243:38349 in memory (size: 28.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,667 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on algo-1:37531 in memory (size: 28.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,670 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on 10.0.143.243:38349 in memory (size: 8.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,672 INFO storage.BlockManagerInfo: Removed broadcast_47_piece0 on algo-1:37531 in memory (size: 8.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,679 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on 10.0.143.243:38349 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,680 INFO storage.BlockManagerInfo: Removed broadcast_42_piece0 on algo-1:37531 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,684 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on 10.0.143.243:38349 in memory (size: 11.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,685 INFO storage.BlockManagerInfo: Removed broadcast_50_piece0 on algo-1:37531 in memory (size: 11.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,692 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on 10.0.143.243:38349 in memory (size: 8.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,693 INFO storage.BlockManagerInfo: Removed broadcast_40_piece0 on algo-1:37531 in memory (size: 8.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,707 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on 10.0.143.243:38349 in memory (size: 47.5 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,707 INFO storage.BlockManagerInfo: Removed broadcast_37_piece0 on algo-1:37531 in memory (size: 47.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,766 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,776 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,833 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,834 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,843 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,862 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,911 WARN nio.NioEventLoop: Selector.select() returned prematurely 512 times in a row; rebuilding Selector io.netty.channel.nio.SelectedSelectionKeySetSelector@2042d48e.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,911 INFO nio.NioEventLoop: Migrated 0 channel(s) to the new Selector.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,921 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,922 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,928 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:24,930 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:25,015 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:25,016 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:25,016 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:25,024 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:25,025 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-2f3fdbbd-a23a-4034-8163-2e8d4283d221\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:25,029 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-2e95341a-423f-48b3-889a-9ec2e07c5ac3\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:25,107 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2026-01-11 23:38:25,107 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n",
      "Baselining job: baseline-suggestion-job-2026-01-11-23-34-22-491\n",
      "Baseline output: s3://i32419/ai-deployment-monitoring-grupo-5/monitoring/baseline-output\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.model_monitor.model_monitoring:Creating Monitoring Schedule with name: aidm-grupo-5-loan-default-dataquality\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created monitoring schedule: aidm-grupo-5-loan-default-dataquality\n",
      "Monitoring outputs: s3://i32419/ai-deployment-monitoring-grupo-5/monitoring/executions\n"
     ]
    }
   ],
   "source": [
    "monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    ")\n",
    "\n",
    "baseline_output_s3 = f\"s3://{bucket}/{prefix}/monitoring/baseline-output\"\n",
    "\n",
    "monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_s3_uri,\n",
    "    dataset_format=DatasetFormat.csv(header=True),\n",
    "    output_s3_uri=baseline_output_s3,\n",
    "    wait=True,\n",
    ")\n",
    "\n",
    "print(\"Baselining job:\", monitor.latest_baselining_job.job_name)\n",
    "print(\"Baseline output:\", baseline_output_s3)\n",
    "\n",
    "schedule_name = \"aidm-grupo-5-loan-default-dataquality\"\n",
    "\n",
    "monitor_output_s3 = f\"s3://{bucket}/{prefix}/monitoring/executions\"\n",
    "\n",
    "monitor.create_monitoring_schedule(\n",
    "    monitor_schedule_name=schedule_name,\n",
    "    endpoint_input=EndpointInput(\n",
    "        endpoint_name=endpoint_name,\n",
    "        destination=\"/opt/ml/processing/input\"\n",
    "    ),\n",
    "    output_s3_uri=monitor_output_s3,\n",
    "    statistics=monitor.baseline_statistics(),\n",
    "    constraints=monitor.suggested_constraints(),\n",
    "    schedule_cron_expression=CronExpressionGenerator.hourly(),\n",
    ")\n",
    "\n",
    "print(\"Created monitoring schedule:\", schedule_name)\n",
    "print(\"Monitoring outputs:\", monitor_output_s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8515d0c-ddb2-46c1-9ecf-24f1794594af",
   "metadata": {},
   "source": [
    "## Simular drift (gerar tráfego “normal” e “drifted”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b301754-9adb-4c4f-9c76-2cad1c5205ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sent 200 normal rows.\n",
      "Sent 200 drifted rows.\n"
     ]
    }
   ],
   "source": [
    "runtime_sm_client = boto3.client(service_name='sagemaker-runtime')\n",
    "\n",
    "df = pd.read_csv(\"data/validation.csv\")\n",
    "X_val = df.drop(columns=[\"Status\"])\n",
    "\n",
    "def invoke_csv_batch(batch_df: pd.DataFrame):\n",
    "    payload = batch_df.to_csv(index=False)\n",
    "    resp = runtime_sm_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType=\"text/csv\",\n",
    "        Body=payload.encode(\"utf-8\"),\n",
    "    )\n",
    "    return resp[\"Body\"].read().decode(\"utf-8\")\n",
    "\n",
    "# 200 linhas normais (4 batches de 50)\n",
    "for i in range(0, 200, 50):\n",
    "    batch = X_val.iloc[i:i+50]\n",
    "    _ = invoke_csv_batch(batch)\n",
    "    time.sleep(0.2)\n",
    "\n",
    "print(\"Sent 200 normal rows.\")\n",
    "\n",
    "# 200 linhas com drift (altera features numéricas)\n",
    "drift_df = X_val.iloc[:200].copy()\n",
    "num_cols = drift_df.select_dtypes(include=[\"number\"]).columns\n",
    "drift_df[num_cols] = drift_df[num_cols] * 3.0 + 100.0\n",
    "\n",
    "for i in range(0, 200, 50):\n",
    "    batch = drift_df.iloc[i:i+50]\n",
    "    _ = invoke_csv_batch(batch)\n",
    "    time.sleep(0.2)\n",
    "\n",
    "print(\"Sent 200 drifted rows.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f40d4b-100d-4a96-97b6-8f73f83a7372",
   "metadata": {},
   "source": [
    "## Verificar execuções do monitor e violações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "90b76565-3c24-48be-b962-77f192db4d80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executions: 1\n",
      "Latest status: Failed\n",
      "Exit message: Error: Encoding mismatch: Encoding is CSV for endpointInput, but Encoding is JSON for endpointOutput. We currently only support the same type of input and output encoding at the moment.\n",
      "Output S3: s3://i32419/ai-deployment-monitoring-grupo-5/monitoring/executions/loan-default-endpoint-2026-01-11-19-51-18/aidm-grupo-5-loan-default-dataquality/2026/01/11/21\n"
     ]
    }
   ],
   "source": [
    "executions = monitor.list_executions()\n",
    "print(\"Executions:\", len(executions))\n",
    "if executions:\n",
    "    latest = executions[-1]\n",
    "    desc = latest.describe()\n",
    "    print(\"Latest status:\", desc.get(\"ProcessingJobStatus\"))\n",
    "    print(\"Exit message:\", desc.get(\"ExitMessage\"))\n",
    "    print(\"Output S3:\", latest.output.destination)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
