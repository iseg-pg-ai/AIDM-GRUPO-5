{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Training with MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook's CI test result for us-west-2 is as follows. CI test results in other regions can be found at the end of the notebook.\n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/ml_ops|sm-mlflow_training|sm-mlflow_training.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import subprocess, json\n",
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "from sagemaker.tuner import HyperparameterTuner, IntegerParameter, ContinuousParameter\n",
    "from datetime import datetime\n",
    "\n",
    "GIT_SHA = subprocess.getoutput(\"git rev-parse --short HEAD\") if os.path.exists(\".git\") else \"unknown\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Declare some variables used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define session, role, and region so we can\n",
    "# perform any SageMaker tasks we need\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "# Bucket S3\n",
    "bucket = \"i32419\"\n",
    "\n",
    "# S3 prefix for the training dataset to be uploaded to\n",
    "prefix = \"ai-deployment-monitoring-grupo-5/aidm-loan-default\"\n",
    "\n",
    "# MLflow\n",
    "tracking_server_arn = \"arn:aws:sagemaker:eu-west-1:267567228900:mlflow-tracking-server/aidm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p training_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get some training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregar o dataset e tratar os valores omissos da variável y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropped 1 rows with NaN Status\n",
      "Unique Status values: [0, 1]\n",
      "Train shape: (26329, 32)\n",
      "Validation shape: (6583, 32)\n",
      "Target distribution (train):\n",
      "Status\n",
      "0    0.756732\n",
      "1    0.243268\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"Dataset/Loan_Default.csv\")\n",
    "\n",
    "# Remover linhas sem target\n",
    "before = len(df)\n",
    "df = df.dropna(subset=[\"Status\"]).copy()\n",
    "after = len(df)\n",
    "print(f\"Dropped {before - after} rows with NaN Status\")\n",
    "\n",
    "# Garantir target binário inteiro\n",
    "df[\"Status\"] = df[\"Status\"].astype(int)\n",
    "print(\"Unique Status values:\", sorted(df[\"Status\"].unique()))\n",
    "\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "# Target binário\n",
    "assert \"Status\" in df.columns, \"Coluna target 'Status' não encontrada\"\n",
    "\n",
    "# Remover identificador (não é feature)\n",
    "if \"ID\" in df.columns:\n",
    "    df = df.drop(columns=[\"ID\"])\n",
    "\n",
    "# Remover year porque o year é sempre 2019\n",
    "if \"year\" in df.columns:\n",
    "    df = df.drop(columns=[\"year\"])\n",
    "\n",
    "# Split train/validation\n",
    "train_df, val_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df[\"Status\"]\n",
    ")\n",
    "\n",
    "train_path = \"./data/train.csv\"\n",
    "val_path = \"./data/validation.csv\"\n",
    "train_df.to_csv(train_path, index=False)\n",
    "val_df.to_csv(val_path, index=False)\n",
    "\n",
    "print(\"Train shape:\", train_df.shape)\n",
    "print(\"Validation shape:\", val_df.shape)\n",
    "print(\"Target distribution (train):\")\n",
    "print(train_df[\"Status\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's upload that data to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train S3: s3://sagemaker-eu-west-1-267567228900/ai-deployment-monitoring-grupo-5/aidm-loan-default/data/train/train.csv\n",
      "Validation S3: s3://sagemaker-eu-west-1-267567228900/ai-deployment-monitoring-grupo-5/aidm-loan-default/data/validation/validation.csv\n"
     ]
    }
   ],
   "source": [
    "WORK_DIRECTORY = \"data\"\n",
    "\n",
    "train_s3_uri = sagemaker_session.upload_data(\n",
    "    path=\"./data/train.csv\",\n",
    "    bucket=bucket,\n",
    "    key_prefix=f\"{prefix}/{WORK_DIRECTORY}/train\"\n",
    ")\n",
    "\n",
    "val_s3_uri = sagemaker_session.upload_data(\n",
    "    path=\"./data/validation.csv\",\n",
    "    bucket=bucket,\n",
    "    key_prefix=f\"{prefix}/{WORK_DIRECTORY}/validation\"\n",
    ")\n",
    "\n",
    "train_input = TrainingInput(train_s3_uri, content_type=\"text/csv\")\n",
    "validation_input = TrainingInput(val_s3_uri, content_type=\"text/csv\")\n",
    "\n",
    "print(\"Train S3:\", train_s3_uri)\n",
    "print(\"Validation S3:\", val_s3_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write your training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write the code to train a Decision Tree model using the scikit-learn framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_code/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile training_code/train.py\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import mlflow\n",
    "\n",
    "def _load_csv(channel_name: str) -> pd.DataFrame:\n",
    "    base = pathlib.Path(\"/opt/ml/input/data\") / channel_name\n",
    "    csv_files = list(base.glob(\"*.csv\"))\n",
    "    if not csv_files:\n",
    "        raise FileNotFoundError(f\"No CSV found in channel: {base}\")\n",
    "    return pd.read_csv(csv_files[0])\n",
    "\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # XGBoost hyperparameters (keep small for short HPO runs)\n",
    "    parser.add_argument(\"--n_estimators\", type=int, default=200)\n",
    "    parser.add_argument(\"--max_depth\", type=int, default=6)\n",
    "    parser.add_argument(\"--learning_rate\", type=float, default=0.1)\n",
    "    parser.add_argument(\"--subsample\", type=float, default=0.8)\n",
    "    parser.add_argument(\"--colsample_bytree\", type=float, default=0.8)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    train_df = _load_csv(\"train\")\n",
    "    val_df = _load_csv(\"validation\")\n",
    "\n",
    "    if \"Status\" not in train_df.columns:\n",
    "        raise ValueError(\"Target column 'Status' not found\")\n",
    "\n",
    "    y_train = train_df[\"Status\"].astype(int)\n",
    "    X_train = train_df.drop(columns=[\"Status\"])\n",
    "\n",
    "    y_val = val_df[\"Status\"].astype(int)\n",
    "    X_val = val_df.drop(columns=[\"Status\"])\n",
    "\n",
    "    cat_cols = X_train.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "    num_cols = [c for c in X_train.columns if c not in cat_cols]\n",
    "\n",
    "    numeric_transformer = Pipeline(\n",
    "        steps=[(\"imputer\", SimpleImputer(strategy=\"median\"))]\n",
    "    )\n",
    "\n",
    "    categorical_transformer = Pipeline(\n",
    "        steps=[\n",
    "            (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
    "            (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"num\", numeric_transformer, num_cols),\n",
    "            (\"cat\", categorical_transformer, cat_cols),\n",
    "        ],\n",
    "        remainder=\"drop\",\n",
    "    )\n",
    "\n",
    "    clf = XGBClassifier(\n",
    "        n_estimators=args.n_estimators,\n",
    "        max_depth=args.max_depth,\n",
    "        learning_rate=args.learning_rate,\n",
    "        subsample=args.subsample,\n",
    "        colsample_bytree=args.colsample_bytree,\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"auc\",\n",
    "        tree_method=\"hist\",\n",
    "        n_jobs=1,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    model = Pipeline(\n",
    "        steps=[\n",
    "            (\"preprocess\", preprocessor),\n",
    "            (\"clf\", clf),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    tracking_arn = os.environ.get(\"MLFLOW_TRACKING_ARN\")\n",
    "    if not tracking_arn:\n",
    "        raise ValueError(\"Missing env var MLFLOW_TRACKING_ARN\")\n",
    "\n",
    "    mlflow.set_tracking_uri(tracking_arn)\n",
    "    mlflow.set_experiment(os.environ.get(\"MLFLOW_EXPERIMENT_NAME\", \"grupo-5-aidm-loan-default\"))\n",
    "\n",
    "    sm_env = json.loads(os.environ.get(\"SM_TRAINING_ENV\", \"{}\"))\n",
    "    training_job_name = sm_env.get(\"job_name\", \"unknown\")\n",
    "\n",
    "    with mlflow.start_run(run_name=training_job_name):\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        val_proba = model.predict_proba(X_val)[:, 1]\n",
    "        val_pred = (val_proba >= 0.5).astype(int)\n",
    "\n",
    "        auc = roc_auc_score(y_val, val_proba)\n",
    "        acc = accuracy_score(y_val, val_pred)\n",
    "\n",
    "        mlflow.log_params(\n",
    "            {\n",
    "                \"n_estimators\": args.n_estimators,\n",
    "                \"max_depth\": args.max_depth,\n",
    "                \"learning_rate\": args.learning_rate,\n",
    "                \"subsample\": args.subsample,\n",
    "                \"colsample_bytree\": args.colsample_bytree,\n",
    "                \"model_type\": \"sklearn_pipeline_xgbclassifier\",\n",
    "                \"num_features\": len(num_cols),\n",
    "                \"cat_features\": len(cat_cols),\n",
    "            }\n",
    "        )\n",
    "        mlflow.log_metrics({\"validation_auc\": float(auc), \"validation_accuracy\": float(acc)})\n",
    "\n",
    "        mlflow.set_tags(\n",
    "            {\n",
    "                \"training_job_name\": training_job_name,\n",
    "                \"dataset\": \"Loan_Default.csv\",\n",
    "                \"task\": \"binary_classification\",\n",
    "                \"target\": \"Status\",\n",
    "                \"git_sha\": os.environ.get(\"GIT_SHA\", \"unknown\"),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        metrics_path = \"/opt/ml/output/metrics.json\"\n",
    "        os.makedirs(os.path.dirname(metrics_path), exist_ok=True)\n",
    "        with open(metrics_path, \"w\") as f:\n",
    "            json.dump({\"validation_auc\": float(auc), \"validation_accuracy\": float(acc)}, f)\n",
    "        mlflow.log_artifact(metrics_path)\n",
    "\n",
    "        model_dir = os.environ.get(\"SM_MODEL_DIR\", \"/opt/ml/model\")\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        joblib.dump(model, os.path.join(model_dir, \"model.joblib\"))\n",
    "\n",
    "        print(f\"validation_auc: {auc}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using MLflow in our training script, let's make sure the container installs `mlflow` along with our MLflow AWS plugin before running our training script. We can do this by creating a `requirements.txt` file and putting it in the same directory as our training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_code/requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile training_code/requirements.txt\n",
    "mlflow==2.13.2\n",
    "sagemaker-mlflow==0.1.0\n",
    "xgboost==2.0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Training and MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your Decision tree model by launching a SageMaker Training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = f\"s3://{bucket}/{prefix}/training-output\"\n",
    "\n",
    "sklearn = SKLearn(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"training_code\",\n",
    "    framework_version=\"1.2-1\",\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters={\n",
    "        \"n_estimators\": 200,\n",
    "        \"max_depth\": 6,\n",
    "        \"learning_rate\": 0.1,\n",
    "        \"subsample\": 0.8,\n",
    "        \"colsample_bytree\": 0.8,\n",
    "    },\n",
    "    keep_alive_period_in_seconds=3600,\n",
    "    environment={\n",
    "        \"MLFLOW_TRACKING_ARN\": tracking_server_arn,\n",
    "        \"MLFLOW_EXPERIMENT_NAME\": \"grupo-5-aidm-loan-default\",\n",
    "        \"GIT_SHA\": GIT_SHA,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "timestamp = datetime.utcnow().strftime(\"%Y%m%d-%H%M%S\")\n",
    "job_name = f\"grupo-5-aidm-loan-default-{timestamp}\"\n",
    "\n",
    "sklearn.fit({\"train\": train_input, \"validation\": validation_input},\n",
    "           job_name=job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimator para HPO (reutiliza o mesmo train.py e source_dir)\n",
    "hpo_estimator = SKLearn(\n",
    "    entry_point=\"train.py\",\n",
    "    source_dir=\"training_code\",\n",
    "    framework_version=\"1.2-1\",\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    output_path=output_path,\n",
    "    keep_alive_period_in_seconds=3600,\n",
    "    environment={\n",
    "        \"MLFLOW_TRACKING_ARN\": tracking_server_arn,\n",
    "        \"MLFLOW_EXPERIMENT_NAME\": \"grupo-5-aidm-loan-default\",\n",
    "        \"GIT_SHA\": GIT_SHA,\n",
    "    },\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir como o tuner “encontra” a métrica nos logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_definitions = [\n",
    "    {\"Name\": \"validation_auc\", \"Regex\": r\"validation_auc:\\s*([0-9\\.]+)\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definir o espaço de hiperparâmetros (curto e realista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    \"n_estimators\": IntegerParameter(50, 300),\n",
    "    \"max_depth\": IntegerParameter(3, 10),\n",
    "    \"learning_rate\": ContinuousParameter(0.03, 0.3),\n",
    "    \"subsample\": ContinuousParameter(0.6, 1.0),\n",
    "    \"colsample_bytree\": ContinuousParameter(0.6, 1.0),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Criar o tuner (job curto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(\n",
    "    estimator=hpo_estimator,\n",
    "    objective_metric_name=\"validation_auc\",\n",
    "    hyperparameter_ranges=hyperparameter_ranges,\n",
    "    metric_definitions=metric_definitions,\n",
    "    objective_type=\"Maximize\",\n",
    "    max_jobs=10,\n",
    "    max_parallel_jobs=2,\n",
    "    base_tuning_job_name=f\"grupo-5-aidm-loan-default-hpo-{timestamp}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lançar o tuning job usando os mesmos channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:sagemaker.estimator:No finished training job found associated with this estimator. Please make sure this estimator is only used for building workflow config\n",
      "INFO:sagemaker:Creating hyperparameter tuning job with name: aidm-loan-default-hp-260111-0134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................................!\n"
     ]
    }
   ],
   "source": [
    "tuner.fit({\"train\": train_input, \"validation\": validation_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obter o melhor job e o melhor modelo (para Registry/BYOC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tuner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best_training_job \u001b[38;5;241m=\u001b[39m \u001b[43mtuner\u001b[49m\u001b[38;5;241m.\u001b[39mbest_training_job()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest training job:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_training_job)\n\u001b[1;32m      4\u001b[0m best_model_s3 \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mbest_estimator()\u001b[38;5;241m.\u001b[39mmodel_data\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tuner' is not defined"
     ]
    }
   ],
   "source": [
    "best_training_job = tuner.best_training_job()\n",
    "print(\"Best training job:\", best_training_job)\n",
    "\n",
    "best_model_s3 = tuner.best_estimator().model_data\n",
    "print(\"Best model artifact:\", best_model_s3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "3b41de70bedc0e302a3aeb58a0c77b854f2e56c8930e61a4aaa3340c96b01f1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
